{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d224ad6-cc67-4a76-b0fa-f7aa2624eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4d1b32-d3de-45e1-a774-e4e2eb0a9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_best(indep, dep, n):\n",
    "    \n",
    "    from sklearn.feature_selection import SelectKBest, chi2\n",
    "    skb=SelectKBest(score_func=chi2, k=n)\n",
    "    \n",
    "    # fit the selectkbest object to the data\n",
    "    skb_fit = skb.fit(indep, dep)\n",
    "    \n",
    "    # trasform the data to the Select top  K features\n",
    "    x_new = skb_fit.transform(indep)\n",
    "    \n",
    "    # Get the names of the selected features...\n",
    "    selected_features = indep.columns[skb.get_support()]\n",
    "    \n",
    "    return selected_features, x_new\n",
    "\n",
    "def standard_scalar(xtrain, xtest):\n",
    "    ### standard scalar\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scx = StandardScaler()\n",
    "    x_train_scaled = scx.fit_transform(xtrain)\n",
    "    x_test_scaled = scx.fit_transform (xtest)\n",
    "    return x_train_scaled, x_test_scaled\n",
    "\n",
    "def metrices(ytest, y_pred):\n",
    "    # making the confusion matrix, classification_report, accuracy_score\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "    cm = confusion_matrix(ytest, y_pred)\n",
    "    clf_report = classification_report(ytest, y_pred, zero_division=1, output_dict=True)\n",
    "    acc_score = accuracy_score(ytest, y_pred)\n",
    "    return cm , clf_report, acc_score\n",
    "    \n",
    "def hyperparameter_tuning(clf_base, param_grid, x_train_scaled, ytrain, ytest):\n",
    "    classifier =GridSearchCV(clf_base, param_grid, refit=True, scoring='f1_weighted',verbose=3, n_jobs=-1,cv=5, error_score=np.nan )\n",
    "    \n",
    "    \n",
    "    model =classifier.fit(x_train_scaled,ytrain)\n",
    "    y_pred = model.predict(x_test_scaled)\n",
    "    \n",
    "    # Get the best Hyperparameters\n",
    "    print(\"best_parameter=\",classifier.best_params_)\n",
    "    print(\"best_estimator=\",classifier.best_estimator_)\n",
    "\n",
    "    return classifier.best_estimator_, classifier.best_params_\n",
    "    \n",
    "#### 1. logistic regression    \n",
    "def logistic(x_train_scaled, x_test_scaled, ytrain, ytest):\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import GridSearchCV  \n",
    "    \n",
    "    # define the base LogiR model\n",
    "    clf_base =LogisticRegression(random_state=0)\n",
    "\n",
    "    # LGR hyperparameter tuning \n",
    "    param_grid= [ {'penalty':[\"l2\"], 'solver':['lbfgs', 'saga'], 'class_weight':['balanced'] },\n",
    "                  {'penalty': [\"l1\"], 'solver': [\"liblinear\", \"saga\"], 'class_weight':['balanced'] } \n",
    "                ]\n",
    "    best_lg, best_lg_hyperparams_ =hyperparameter_tuning(clf_base, param_grid, x_train_scaled, ytrain, ytest)\n",
    "    \n",
    "    y_pred= best_lg.predict(xtest)\n",
    "    \n",
    "    cm,clf_report, acc_score = metrices(ytest, y_pred)\n",
    "    return cm , clf_report, acc_score ,best_lg, best_lg_hyperparams_ ,y_pred\n",
    "    \n",
    "### 2. SVM_linear\n",
    "\n",
    "def svm_linear(x_train_scaled, x_test_scaled, ytrain, ytest): \n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # define the base SVMl model\n",
    "    clf_base = SVC(random_state=0)\n",
    "    \n",
    "    # SVML hyperparameter tuning \n",
    "    param_grid= {'kernel':[\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "             'C':[10,100,1000,2000,3000],\n",
    "             'gamma':['auto','scale']\n",
    "             }\n",
    "    best_svml, best_svml_hyperparams_ =hyperparameter_tuning(clf_base, param_grid, x_train_scaled, ytrain, ytest)\n",
    "    \n",
    "    y_pred= best_svml.predict(xtest)\n",
    "    \n",
    "    cm,clf_report, acc_score = metrices(ytest, y_pred)\n",
    "    return cm , clf_report, acc_score ,best_svml, best_svml_hyperparams_,y_pred\n",
    "    \n",
    "    \n",
    "### 4. Decision Tree\n",
    "def decision(x_train_scaled, x_test_scaled, ytrain, ytest):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    \n",
    "    # define the base Decision model\n",
    "    clf_base= DecisionTreeClassifier(random_state=0)\n",
    "    \n",
    "    # DecisionTree hyperparameter tuning\n",
    "    param_grid= {'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "             'splitter':['best', 'random'],\n",
    "             'max_features':['sqrt', 'log2'],\n",
    "             \n",
    "            }\n",
    "    best_dt, best_dt_hyperparams_ =hyperparameter_tuning(clf_base, param_grid, x_train_scaled, ytrain, ytest)\n",
    "    \n",
    "    y_pred= best_dt.predict(xtest)\n",
    "    \n",
    "    cm,clf_report, acc_score = metrices(ytest, y_pred)\n",
    "    return cm , clf_report, acc_score ,best_dt, best_dt_hyperparams_,y_pred\n",
    "    \n",
    "\n",
    "### 5. RandomForest\n",
    "def random(x_train_scaled, x_test_scaled, ytrain, ytest):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    # define the base random model\n",
    "    clf_base= RandomForestClassifier(random_state=0)\n",
    "    \n",
    "    # RandomTree hyperparameter tuning\n",
    "    param_grid= {'n_estimators':[10],\n",
    "             'criterion':['gini', 'entropy', 'log_loss'],\n",
    "             'max_features':['sqrt', 'log2', None],\n",
    "             \n",
    "            }\n",
    "    best_rf, best_rf_hyperparams_ =hyperparameter_tuning(clf_base, param_grid, x_train_scaled, ytrain, ytest)\n",
    "    \n",
    "    y_pred= best_rf.predict(xtest)\n",
    "    \n",
    "    cm,clf_report, acc_score = metrices(ytest, y_pred)\n",
    "    return cm , clf_report, acc_score ,best_rf, best_rf_hyperparams_,y_pred \n",
    "    \n",
    "\n",
    "## 6. KNN (KNearestNeighbours)\n",
    "\n",
    "##Power parameter for the Minkowski metric. When p = 1, \n",
    "#####   this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n",
    "######    For arbitrary p, minkowski_distance (l_p) is used.\n",
    "## Metric to use for distance computation. Default is \"minkowski\", which\n",
    "##### results in the standard Euclidean distance when p = 2. \n",
    "def knn(x_train_scaled, x_test_scaled, ytrain, ytest):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    \n",
    "    # define the base random model\n",
    "    clf_base= KNeighborsClassifier()\n",
    "    \n",
    "    # RandomTree hyperparameter tuning\n",
    "    param_grid= {'n_neighbors':[1,12],\n",
    "                  'p': [1,2]       \n",
    "            }\n",
    "    best_knn, best_knn_hyperparams_ =hyperparameter_tuning(clf_base, param_grid, x_train_scaled, ytrain, ytest )\n",
    "    y_pred= best_knn.predict(xtest)\n",
    "    \n",
    "    cm,clf_report, acc_score = metrices(ytest, y_pred)\n",
    "    return cm , clf_report, acc_score ,best_knn, best_knn_hyperparams_,y_pred \n",
    "    \n",
    "    \n",
    "### 7. Naive Bayes\n",
    "def naive(x_train_scaled, x_test_scaled, ytrain, ytest):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "    # define the base Naive Bayes model\n",
    "    clf_base = GaussianNB()\n",
    "    \n",
    "    # Naive Bayes hyperparameter tuning\n",
    "     # n_jobs is the maximum number of concurrently running workers; in this case, #it is set to -1 which implies that all CPUs are used.\n",
    "     # verbose is the verbosity: the higher, the more messages; in this case, it is set to 1.\n",
    "    param_grid= { 'var_smoothing': [1e-9, 1e-8, 1e-7]  }\n",
    "\n",
    "    best_nb, best_nb_hyperparams_ =hyperparameter_tuning(clf_base, param_grid, x_train_scaled, ytrain, ytest )\n",
    "    y_pred= best_nb.predict(xtest)\n",
    "    \n",
    "    cm,clf_report, acc_score = metrices(ytest, y_pred)\n",
    "    return cm , clf_report, acc_score ,best_nb, best_nb_hyperparams_ ,y_pred\n",
    "\n",
    "#### display the accuracy in a table\n",
    "\n",
    "def view_acc_score(acc_logistic,acc_svmlinear,acc_decision,acc_random,acc_knn,acc_naive):\n",
    "\n",
    "    table= pd.DataFrame(index=[\"chi square\"], columns=[\"Logistic\",\"svmlinear\",\"decision\",\"random\",\"knn\",\"naive\"])\n",
    "    # Loop through each index label and assign values from the lists to the corresponding row\n",
    "    for i, label in enumerate(table.index):\n",
    "        table.loc[label] = [\n",
    "            acc_logistic[i], \n",
    "            acc_svmlinear[i],  \n",
    "            acc_decision[i], \n",
    "            acc_random[i], \n",
    "            acc_knn[i], \n",
    "            acc_naive[i]\n",
    "    ]\n",
    "\n",
    "    return table\n",
    "\n",
    "#def evaluating_metrics(clf_report,ytest, y_pred, model_name):\n",
    "    # extracting metrics\n",
    "#    metrics= {\n",
    "#        \"precision_0\":clf_report[\"0\"][\"precision\"],\n",
    "#        'precision_1':clf_report[\"1\"]['precision'],\n",
    "#        'recall_0':clf_report['0']['recall'],\n",
    "#        'recall_1':clf_report['1']['recall'],\n",
    "#        'f1_score_0':clf_report['0']['f1-score'],\n",
    "#        'f1_score_1':clf_report['1']['f1-score'],\n",
    "#        'mac_average_precision':clf_report['macro avg']['precision'],\n",
    "#        'mac_average_recall':clf_report['macro avg']['recall'],\n",
    "#        'mac_average_f1_score':clf_report['macro avg']['f1-score'],\n",
    "        \n",
    "#        \"accuracy_p\":clf_report['accuracy'],\n",
    "#        \"accuracy_r\":clf_report['accuracy'],\n",
    "#        \"accuracy_f\":clf_report['accuracy']\n",
    "#    }\n",
    "#    #convert this dictionary into Dataframe\n",
    "#    df =pd.DataFrame(metrics, index=[model_name]).round(2)\n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aca3723d-0254-4ac5-bc7c-f5267e56cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv(\"prep_ds.csv\")\n",
    "#dataset\n",
    "\n",
    "### separation of indep and dep vars\n",
    "\n",
    "indep = dataset.drop(\"HeartDisease\", axis=1)\n",
    "dep = dataset[\"HeartDisease\"]\n",
    "\n",
    "# Applyting the SelectKbest...\n",
    "selected_features, x_new = select_k_best(indep, dep, 3)\n",
    "\n",
    "### Separation of Training and Test dataset...\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x_new, dep, test_size=0.30, random_state=1)\n",
    "\n",
    "# Standard scalar\n",
    "x_train_scaled, x_test_scaled = standard_scalar(xtrain, xtest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e587620-50a0-4a95-b95f-81330a91f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "best_parameter= {'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "best_estimator= LogisticRegression(class_weight='balanced', random_state=0)\n",
      "LG Optimal Hyper parameters:  {'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "acc_logistic= []\n",
    "acc_svmlinear = []\n",
    "acc_svm_nonlinear = []\n",
    "acc_decision = []\n",
    "acc_random = []\n",
    "acc_knn = []\n",
    "acc_naive = []\n",
    "\n",
    "\n",
    "cm ,clf_report,acc_score,best_lg, best_lg_hyperparams_,y_pred= logistic(x_train_scaled, x_test_scaled, ytrain, ytest)\n",
    "print(\"LG Optimal Hyper parameters: \",best_lg_hyperparams_)\n",
    "acc_logistic.append(acc_score)\n",
    "#df =evaluating_metrics(clf_report, ytest, y_pred, 'LR')\n",
    "#df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6db71fc3-9444-4a54-a632-600c2b6878b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "best_parameter= {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "best_estimator= SVC(C=10, gamma='auto', random_state=0)\n",
      "SVML Optimal Hyper parameters:  {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "cm ,clf_report,acc_score, best_svml, best_svml_hyperparams_,y_pred= svm_linear(x_train_scaled, x_test_scaled, ytrain, ytest)\n",
    "print(\"SVML Optimal Hyper parameters: \",best_svml_hyperparams_)\n",
    "acc_svmlinear.append(acc_score)\n",
    "#df =evaluating_metrics(clf_report, ytest, y_pred, 'svml')\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27f335b8-324d-423c-83a1-0bfbd394819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "best_parameter= {'criterion': 'gini', 'max_features': 'sqrt', 'splitter': 'random'}\n",
      "best_estimator= DecisionTreeClassifier(max_features='sqrt', random_state=0, splitter='random')\n",
      "DecisionTree Optimal Hyper parameters:  {'criterion': 'gini', 'max_features': 'sqrt', 'splitter': 'random'}\n"
     ]
    }
   ],
   "source": [
    "cm ,clf_report,acc_score, best_dt, best_dt_hyperparams_,y_pred= decision(x_train_scaled, x_test_scaled, ytrain, ytest)\n",
    "print(\"DecisionTree Optimal Hyper parameters: \",best_dt_hyperparams_)\n",
    "acc_decision.append(acc_score)\n",
    "#df =evaluating_metrics(clf_report, ytest, y_pred, 'decision')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "468737ba-d40c-44fd-bcab-89e66eaadc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "best_parameter= {'criterion': 'gini', 'max_features': None, 'n_estimators': 10}\n",
      "best_estimator= RandomForestClassifier(max_features=None, n_estimators=10, random_state=0)\n",
      "RandomForest Optimal Hyper parameters:  {'criterion': 'gini', 'max_features': None, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "cm ,clf_report,acc_score, best_rf, best_rf_hyperparams_ ,y_pred= random(x_train_scaled, x_test_scaled, ytrain, ytest)\n",
    "print(\"RandomForest Optimal Hyper parameters: \",best_rf_hyperparams_)\n",
    "acc_random.append(acc_score)\n",
    "#df =evaluating_metrics(clf_report, ytest, y_pred, 'random')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "926d3d71-e832-471e-89ad-4cb903c62435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "best_parameter= {'n_neighbors': 12, 'p': 1}\n",
      "best_estimator= KNeighborsClassifier(n_neighbors=12, p=1)\n",
      "KNN Optimal Hyper parameters:  {'n_neighbors': 12, 'p': 1}\n"
     ]
    }
   ],
   "source": [
    "cm ,clf_report,acc_score, best_knn, best_knn_hyperparams_,y_pred= knn(x_train_scaled, x_test_scaled, ytrain, ytest)\n",
    "print(\"KNN Optimal Hyper parameters: \",best_knn_hyperparams_)\n",
    "acc_knn.append(acc_score)\n",
    "#df =evaluating_metrics(clf_report, ytest, y_pred, 'KNN')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bffa41b-3064-457c-9227-2d4edad0c23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "best_parameter= {'var_smoothing': 1e-09}\n",
      "best_estimator= GaussianNB()\n",
      "Naive Bayes Optimal Hyper parameters:  {'var_smoothing': 1e-09}\n"
     ]
    }
   ],
   "source": [
    "cm ,clf_report,acc_score, best_nb, best_nb_hyperparams_,y_pred= naive(x_train_scaled, x_test_scaled, ytrain, ytest)\n",
    "print(\"Naive Bayes Optimal Hyper parameters: \",best_nb_hyperparams_)\n",
    "acc_naive.append(acc_score)\n",
    "#df =evaluating_metrics(clf_report, ytest, y_pred, 'Naive Bayes')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7175ef7-d26b-44c7-9a4a-bbd86aadbbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:  Index(['MaxHR', 'ST_Slope_Flat', 'ST_Slope_Up'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#### selected features:\n",
    "\n",
    "print (\"Selected Features: \", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9df837d2-827d-4f30-8ecf-3f1bed5de042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f1660c7-30ac-42bb-b60b-da18654bb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Disply the accuracy score in a table\n",
    "\n",
    "table =view_acc_score(acc_logistic,acc_svmlinear,acc_decision,acc_random,acc_knn,acc_naive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76e411db-abd9-4db6-8bbc-051eede499c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic</th>\n",
       "      <th>svmlinear</th>\n",
       "      <th>decision</th>\n",
       "      <th>random</th>\n",
       "      <th>knn</th>\n",
       "      <th>naive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chi square</th>\n",
       "      <td>0.394928</td>\n",
       "      <td>0.394928</td>\n",
       "      <td>0.605072</td>\n",
       "      <td>0.76087</td>\n",
       "      <td>0.76087</td>\n",
       "      <td>0.605072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Logistic svmlinear  decision   random      knn     naive\n",
       "chi square  0.394928  0.394928  0.605072  0.76087  0.76087  0.605072"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59c265d8-887c-433e-aaef-cc5b9dfb6f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic</th>\n",
       "      <th>svmlinear</th>\n",
       "      <th>decision</th>\n",
       "      <th>random</th>\n",
       "      <th>knn</th>\n",
       "      <th>naive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chi square</th>\n",
       "      <td>0.394928</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.648551</td>\n",
       "      <td>0.76087</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.605072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Logistic svmlinear  decision   random       knn     naive\n",
       "chi square  0.394928  0.536232  0.648551  0.76087  0.724638  0.605072"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34f19241-6d66-4d15-81a7-061ab6a6ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conclusion: Accuracy score for Random forest and KNN is higher when selectkBest_K=3\n",
    "###             so we will save any one of these two model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a9d06-ed8c-4ee5-bcc0-f526ba443785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
